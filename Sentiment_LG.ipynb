{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFj298LC2q8H",
        "outputId": "1c80a087-9496-4223-ccb6-7f7e5d9ece1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.19.1-py3-none-any.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.10.0 (from gradio)\n",
            "  Downloading gradio_client-0.10.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.1.7 (from gradio)\n",
            "  Downloading ruff-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.9.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.10.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.10.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->gradio)\n",
            "  Downloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=8718acbbc0cf774a7da4f1340635031b16ad840e258583ddc3c5c5a9617bc225\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.109.2 ffmpy-0.3.2 gradio-4.19.1 gradio-client-0.10.0 h11-0.14.0 httpcore-1.0.3 httpx-0.26.0 orjson-3.9.14 pydub-0.25.1 python-multipart-0.0.9 ruff-0.2.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 uvicorn-0.27.1 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG9eugOTJYNO"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import nltk, re, string\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1FrZTtEXOei"
      },
      "outputs": [],
      "source": [
        "# For better understanding of Logistic Regression: https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkaGbJoUJoOC"
      },
      "outputs": [],
      "source": [
        "# Preprocessing of the tweets that is our data\n",
        "\n",
        "# def process_tweet(tweet):\n",
        "#     stemmer = nltk.PorterStemmer()\n",
        "#     stopwords_english = stopwords.words('english')\n",
        "#     tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "#     tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "#     tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "#     tweet = re.sub(r'#', '', tweet)\n",
        "#     tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "#     tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "#     tweets_clean = []\n",
        "#     for word in tweet_tokens:\n",
        "#         if (word not in stopwords_english and\n",
        "#                 word not in string.punctuation):\n",
        "#             stem_word = stemmer.stem(word)  # stemming word\n",
        "#             tweets_clean.append(stem_word)\n",
        "\n",
        "#     return tweets_clean\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to clean the tweets\n",
        "def process_tweet(text):\n",
        "    stemmer = nltk.PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    tweet = re.sub(r'@[A-Za-z0-9]+', '', text) # Removing @mentions #Removing @mentions\n",
        "    tweet = re.sub(r'#+', '', text) # Removing '#' hash tag\n",
        "    tweet = re.sub(r'RT[\\s]+', '', text) # Removing RT\n",
        "    tweet = re.sub(r'(http|https):\\/\\/\\S+', '', text) # Removing hyperlink\n",
        "    tweet = re.sub(r\"^b[\\'$]\", '', text) # b'\n",
        "    tweet = re.sub(r\"\\:[\\s]\", ' ', text) # :\n",
        "    tweet = re.sub(r\"\\+\", '', text) # +\n",
        "    tweet = re.sub(r\"F\\@\\$[\\s]+\", '', text) #F@$\n",
        "    tweet = re.sub(r\"\\\\\\[nxf09f928ce29ca89f8cb7]+\", '', text) #\\\n",
        "    tweet = re.sub(r\"\\!\", '', text) #!\n",
        "    tweet = re.sub(r\"\\&\\[amp]+\", '', text) #&\n",
        "    tweet = re.sub(r\"\\;\", '', text) #;\n",
        "    tweet = re.sub(r\"\\_\", '', text) #_\n",
        "    tweet = re.sub(r\"\\,\", '', text) #,\n",
        "    tweet = re.sub(r\"\\&\\amp+\", '', text) #&amp\n",
        "    text = re.sub(r\"\\%\", '', text) #%\n",
        "    tweet = re.sub(r\"^\\s\", '', text) #&\n",
        "    # tweet = re.sub(\"TUTORIALhope\", 'TUTORIAL hope', text) #spaces\n",
        "    tweet = re.sub(r\"\\&[amp\\s]+\", '', text) #&\n",
        "    tweet = re.sub(r\"\\.+\", '', text) #.\n",
        "    tweet = re.sub(r\"\\/\\/\", '', text) #//\n",
        "    tweet = re.sub(r\"\\|\", '', text) #|\n",
        "    tweet = re.sub(r\"^b\\\"\", '', text) #.\n",
        "    tweet = re.sub(r\"\\'\", '', text) #.\n",
        "    tweet = re.sub(r\"\\?\", '', text) #.\n",
        "    tweet = re.sub(r\"\\:\", '', text) #.\n",
        "    tweet = re.sub(r'\\d+', '', text)#digits\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)#emojies\n",
        "    tweet = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "    tweet = re.sub(r'\\b\\d+\\b', '', tweet)\n",
        "    # tweet = re.sub(r\"\\:)\", '', text) #:)\n",
        "    # text = re.sub(\"mexicanaLa\", 'mexicana La', text) #spaces\n",
        "    tweet = re.sub(r'\\w+\\d+\\w+', '', text) #Removing @mentions\n",
        "    tweet = re.sub(r'\\w+\\d+', '', text) #Removing @mentions\n",
        "    tweet = re.sub(r'https?://t\\.co/\\w+', '', text)\n",
        "    tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and\n",
        "                word not in string.punctuation and len(word)>3):\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n"
      ],
      "metadata": {
        "id": "ifzTO2no5aHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKakfgACJwQL"
      },
      "outputs": [],
      "source": [
        "# This is the most important part of the whole code:\n",
        "# The reason is our feature set through which we will be training our model on will be build here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPTgml3SJ0pJ"
      },
      "outputs": [],
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aKmKS4-KJvp",
        "outputId": "75c690bb-0bb0-4378-a8fb-108d217f35c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y61jYX3fUkkU",
        "outputId": "507ca15f-5941-4457-d4d6-91d0b9747144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('happi', 1): 1, ('trick', 0): 1, ('tire', 0): 2}\n"
          ]
        }
      ],
      "source": [
        "# Checking how the above code works with an example.\n",
        "\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "res = build_freqs(tweets, ys)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxViCeI-KDN7"
      },
      "outputs": [],
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(all_positive_tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNSe2OFP-7UB",
        "outputId": "80a1bff4-61cb-4cb7-9d35-8fce4f111e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03NIPdv1KF96"
      },
      "outputs": [],
      "source": [
        "# split the data into two pieces, one for training and one for testing.\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOGG2-3WKRQ7"
      },
      "outputs": [],
      "source": [
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0qfTKjFLksh"
      },
      "outputs": [],
      "source": [
        "train_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGQdgF5VKTia"
      },
      "outputs": [],
      "source": [
        "# combine positive and negative labels\n",
        "# We are building our y - target variable here\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVDUs1paKb4z"
      },
      "outputs": [],
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0kedQP6RCCt",
        "outputId": "09d97e2f-02c0-4548-fd96-3dfa12d3b33d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('#followfriday', 1.0): 23,\n",
              " ('engag', 1.0): 7,\n",
              " ('member', 1.0): 14,\n",
              " ('commun', 1.0): 26,\n",
              " ('week', 1.0): 72,\n",
              " ('jame', 1.0): 7,\n",
              " ('pleas', 1.0): 81,\n",
              " ('call', 1.0): 27,\n",
              " ('contact', 1.0): 4,\n",
              " ('centr', 1.0): 1,\n",
              " ('02392441234', 1.0): 1,\n",
              " ('abl', 1.0): 6,\n",
              " ('assist', 1.0): 1,\n",
              " ('mani', 1.0): 28,\n",
              " ('thank', 1.0): 522,\n",
              " ('listen', 1.0): 15,\n",
              " ('last', 1.0): 39,\n",
              " ('night', 1.0): 55,\n",
              " ('bleed', 1.0): 2,\n",
              " ('amaz', 1.0): 41,\n",
              " ('track', 1.0): 5,\n",
              " ('scotland', 1.0): 2,\n",
              " ('congrat', 1.0): 15,\n",
              " ('yeaaah', 1.0): 1,\n",
              " ('yipppi', 1.0): 1,\n",
              " ('accnt', 1.0): 2,\n",
              " ('verifi', 1.0): 2,\n",
              " ('rqst', 1.0): 1,\n",
              " ('succeed', 1.0): 1,\n",
              " ('blue', 1.0): 8,\n",
              " ('tick', 1.0): 1,\n",
              " ('mark', 1.0): 1,\n",
              " ('profil', 1.0): 2,\n",
              " ('day', 1.0): 30,\n",
              " ('irresist', 1.0): 2,\n",
              " ('#flipkartfashionfriday', 1.0): 16,\n",
              " ('like', 1.0): 187,\n",
              " ('keep', 1.0): 55,\n",
              " ('love', 1.0): 334,\n",
              " ('custom', 1.0): 4,\n",
              " ('wait', 1.0): 55,\n",
              " ('long', 1.0): 27,\n",
              " ('hope', 1.0): 115,\n",
              " ('enjoy', 1.0): 60,\n",
              " ('happi', 1.0): 157,\n",
              " ('friday', 1.0): 82,\n",
              " ('lwwf', 1.0): 1,\n",
              " ('second', 1.0): 8,\n",
              " ('thought', 1.0): 21,\n",
              " ('there', 1.0): 4,\n",
              " ('enough', 1.0): 16,\n",
              " ('time', 1.0): 101,\n",
              " ('short', 1.0): 6,\n",
              " ('enter', 1.0): 9,\n",
              " ('system', 1.0): 2,\n",
              " ('sheep', 1.0): 1,\n",
              " ('must', 1.0): 14,\n",
              " ('buy', 1.0): 1,\n",
              " ('bayan', 1.0): 1,\n",
              " ('mischiev', 1.0): 1,\n",
              " ('layer', 1.0): 1,\n",
              " ('in-hous', 1.0): 1,\n",
              " ('wareh', 1.0): 1,\n",
              " ('katamari', 1.0): 1,\n",
              " ('well', 1.0): 66,\n",
              " ('name', 1.0): 12,\n",
              " ('impli', 1.0): 1,\n",
              " ('influenc', 1.0): 16,\n",
              " ('juici', 1.0): 3,\n",
              " ('selfi', 1.0): 10,\n",
              " ('follow', 1.0): 382,\n",
              " ('back', 1.0): 139,\n",
              " ('perfect', 1.0): 16,\n",
              " ('alreadi', 1.0): 19,\n",
              " ('know', 1.0): 128,\n",
              " (\"what'\", 1.0): 14,\n",
              " ('great', 1.0): 135,\n",
              " ('opportun', 1.0): 17,\n",
              " ('junior', 1.0): 2,\n",
              " ('triathlet', 1.0): 1,\n",
              " ('age', 1.0): 1,\n",
              " ('gatorad', 1.0): 1,\n",
              " ('seri', 1.0): 4,\n",
              " ('entri', 1.0): 3,\n",
              " ('lay', 1.0): 2,\n",
              " ('greet', 1.0): 4,\n",
              " ('card', 1.0): 6,\n",
              " ('rang', 1.0): 2,\n",
              " ('print', 1.0): 4,\n",
              " ('today', 1.0): 91,\n",
              " (\"friend'\", 1.0): 3,\n",
              " ('lunch', 1.0): 3,\n",
              " ('yummm', 1.0): 1,\n",
              " ('#nostalgia', 1.0): 1,\n",
              " ('#tb', 1.0): 1,\n",
              " ('conflict', 1.0): 1,\n",
              " ('help', 1.0): 39,\n",
              " (\"here'\", 1.0): 20,\n",
              " ('screenshot', 1.0): 2,\n",
              " ('work', 1.0): 89,\n",
              " ('hello', 1.0): 48,\n",
              " ('need', 1.0): 62,\n",
              " ('someth', 1.0): 25,\n",
              " ('twitter', 1.0): 25,\n",
              " ('sure', 1.0): 38,\n",
              " ('thing', 1.0): 48,\n",
              " (\"i'v\", 1.0): 25,\n",
              " ('heard', 1.0): 9,\n",
              " ('four', 1.0): 5,\n",
              " ('season', 1.0): 5,\n",
              " ('pretti', 1.0): 16,\n",
              " ('dope', 1.0): 2,\n",
              " ('penthous', 1.0): 1,\n",
              " ('obv', 1.0): 1,\n",
              " ('#gobigorgohom', 1.0): 1,\n",
              " (\"y'all\", 1.0): 4,\n",
              " ('yeah', 1.0): 30,\n",
              " ('suppos', 1.0): 6,\n",
              " ('chat', 1.0): 7,\n",
              " ('youth', 1.0): 14,\n",
              " ('seen', 1.0): 6,\n",
              " ('year', 1.0): 33,\n",
              " ('rest', 1.0): 9,\n",
              " ('goe', 1.0): 4,\n",
              " ('quickli', 1.0): 3,\n",
              " ('music', 1.0): 13,\n",
              " ('dream', 1.0): 17,\n",
              " ('spiritu', 1.0): 1,\n",
              " ('ritual', 1.0): 1,\n",
              " ('festiv', 1.0): 7,\n",
              " ('npal', 1.0): 1,\n",
              " ('begin', 1.0): 4,\n",
              " ('line-up', 1.0): 4,\n",
              " ('left', 1.0): 10,\n",
              " ('sarah', 1.0): 4,\n",
              " ('send', 1.0): 18,\n",
              " ('email', 1.0): 22,\n",
              " ('bitsy@bitdefender.com', 1.0): 1,\n",
              " (\"we'll\", 1.0): 12,\n",
              " ('asap', 1.0): 5,\n",
              " ('lol', 1.0): 1,\n",
              " ('hatessuc', 1.0): 1,\n",
              " ('32429', 1.0): 1,\n",
              " ('#kik', 1.0): 9,\n",
              " ('#kikm', 1.0): 1,\n",
              " ('#lgbt', 1.0): 2,\n",
              " ('#tinder', 1.0): 1,\n",
              " ('#nsfw', 1.0): 1,\n",
              " ('#akua', 1.0): 1,\n",
              " ('#cumshot', 1.0): 1,\n",
              " ('come', 1.0): 63,\n",
              " ('hous', 1.0): 5,\n",
              " ('#nsn_supplement', 1.0): 1,\n",
              " ('effect', 1.0): 2,\n",
              " ('press', 1.0): 1,\n",
              " ('releas', 1.0): 11,\n",
              " ('distribut', 1.0): 1,\n",
              " ('result', 1.0): 2,\n",
              " ('link', 1.0): 14,\n",
              " ('remov', 1.0): 3,\n",
              " ('#pressreleas', 1.0): 1,\n",
              " ('#newsdistribut', 1.0): 1,\n",
              " ('bestfriend', 1.0): 50,\n",
              " ('warsaw', 1.0): 44,\n",
              " ('everyon', 1.0): 45,\n",
              " ('watch', 1.0): 32,\n",
              " ('documentari', 1.0): 1,\n",
              " ('earthl', 1.0): 1,\n",
              " ('youtub', 1.0): 6,\n",
              " ('support', 1.0): 25,\n",
              " ('buuut', 1.0): 1,\n",
              " ('look', 1.0): 111,\n",
              " ('forward', 1.0): 20,\n",
              " ('visit', 1.0): 25,\n",
              " ('next', 1.0): 37,\n",
              " ('#letsgetmessi', 1.0): 1,\n",
              " ('make', 1.0): 70,\n",
              " ('feel', 1.0): 33,\n",
              " ('better', 1.0): 40,\n",
              " ('never', 1.0): 31,\n",
              " ('anyon', 1.0): 7,\n",
              " ('kpop', 1.0): 1,\n",
              " ('flesh', 1.0): 1,\n",
              " ('good', 1.0): 191,\n",
              " ('girl', 1.0): 33,\n",
              " ('best', 1.0): 49,\n",
              " ('wish', 1.0): 28,\n",
              " ('reason', 1.0): 10,\n",
              " ('epic', 1.0): 1,\n",
              " ('soundtrack', 1.0): 1,\n",
              " ('shout', 1.0): 11,\n",
              " ('ad', 1.0): 8,\n",
              " ('video', 1.0): 30,\n",
              " ('playlist', 1.0): 5,\n",
              " ('twitch', 1.0): 7,\n",
              " ('go', 1.0): 55,\n",
              " ('leagu', 1.0): 6,\n",
              " ('would', 1.0): 70,\n",
              " ('dear', 1.0): 15,\n",
              " ('#jordan', 1.0): 1,\n",
              " ('okay', 1.0): 31,\n",
              " ('fake', 1.0): 1,\n",
              " ('gameplay', 1.0): 1,\n",
              " ('haha', 1.0): 44,\n",
              " ('kid', 1.0): 10,\n",
              " ('stuff', 1.0): 11,\n",
              " ('exactli', 1.0): 5,\n",
              " ('product', 1.0): 11,\n",
              " ('line', 1.0): 6,\n",
              " ('#etsi', 1.0): 1,\n",
              " ('shop', 1.0): 12,\n",
              " ('check', 1.0): 38,\n",
              " ('#boxroomcraft', 1.0): 1,\n",
              " ('vacat', 1.0): 5,\n",
              " ('recharg', 1.0): 1,\n",
              " ('normal', 1.0): 5,\n",
              " ('charger', 1.0): 2,\n",
              " ('asleep', 1.0): 7,\n",
              " ('talk', 1.0): 37,\n",
              " ('sooo', 1.0): 6,\n",
              " ('someon', 1.0): 29,\n",
              " ('text', 1.0): 12,\n",
              " (\"he'll\", 1.0): 2,\n",
              " ('hear', 1.0): 24,\n",
              " ('speech', 1.0): 1,\n",
              " ('piti', 1.0): 2,\n",
              " ('green', 1.0): 2,\n",
              " ('garden', 1.0): 5,\n",
              " ('midnight', 1.0): 1,\n",
              " ('beauti', 1.0): 45,\n",
              " ('canal', 1.0): 1,\n",
              " ('dasvidaniya', 1.0): 1,\n",
              " ('till', 1.0): 16,\n",
              " ('scout', 1.0): 1,\n",
              " ('futur', 1.0): 9,\n",
              " ('wlan', 1.0): 1,\n",
              " ('pro', 1.0): 1,\n",
              " ('confer', 1.0): 1,\n",
              " ('asia', 1.0): 1,\n",
              " ('chang', 1.0): 20,\n",
              " ('lollipop', 1.0): 1,\n",
              " ('#agnezmo', 1.0): 1,\n",
              " ('oley', 1.0): 1,\n",
              " ('mama', 1.0): 1,\n",
              " ('stand', 1.0): 6,\n",
              " ('stronger', 1.0): 1,\n",
              " ('misti', 1.0): 1,\n",
              " ('babi', 1.0): 17,\n",
              " ('cute', 1.0): 19,\n",
              " ('woohoo', 1.0): 3,\n",
              " (\"can't\", 1.0): 31,\n",
              " ('sign', 1.0): 9,\n",
              " ('still', 1.0): 37,\n",
              " ('think', 1.0): 54,\n",
              " ('liam', 1.0): 5,\n",
              " ('access', 1.0): 3,\n",
              " ('welcom', 1.0): 54,\n",
              " ('stat', 1.0): 51,\n",
              " ('arriv', 1.0): 57,\n",
              " ('unfollow', 1.0): 53,\n",
              " ('surpris', 1.0): 10,\n",
              " ('figur', 1.0): 5,\n",
              " ('#happybirthdayemilybett', 1.0): 1,\n",
              " ('sweet', 1.0): 16,\n",
              " ('talent', 1.0): 4,\n",
              " ('plan', 1.0): 21,\n",
              " ('drain', 1.0): 1,\n",
              " ('gotta', 1.0): 4,\n",
              " ('timezon', 1.0): 1,\n",
              " ('parent', 1.0): 3,\n",
              " ('proud', 1.0): 11,\n",
              " ('least', 1.0): 14,\n",
              " ('mayb', 1.0): 17,\n",
              " ('sometim', 1.0): 11,\n",
              " ('grade', 1.0): 4,\n",
              " ('grand', 1.0): 4,\n",
              " ('manila_bro', 1.0): 1,\n",
              " ('chosen', 1.0): 1,\n",
              " ('around', 1.0): 14,\n",
              " ('side', 1.0): 13,\n",
              " ('world', 1.0): 23,\n",
              " ('take', 1.0): 30,\n",
              " ('care', 1.0): 12,\n",
              " ('final', 1.0): 24,\n",
              " ('fuck', 1.0): 19,\n",
              " ('weekend', 1.0): 58,\n",
              " ('real', 1.0): 18,\n",
              " ('join', 1.0): 21,\n",
              " ('#hushedcallwithfraydo', 1.0): 1,\n",
              " ('gift', 1.0): 7,\n",
              " ('yeahhh', 1.0): 1,\n",
              " ('#hushedpinwithsammi', 1.0): 2,\n",
              " ('event', 1.0): 8,\n",
              " ('might', 1.0): 21,\n",
              " ('realli', 1.0): 66,\n",
              " ('appreci', 1.0): 28,\n",
              " ('share', 1.0): 40,\n",
              " ('monday', 1.0): 7,\n",
              " ('invit', 1.0): 15,\n",
              " ('scope', 1.0): 5,\n",
              " ('friend', 1.0): 42,\n",
              " ('nude', 1.0): 1,\n",
              " ('sleep', 1.0): 35,\n",
              " ('birthday', 1.0): 52,\n",
              " ('want', 1.0): 71,\n",
              " ('t-shirt', 1.0): 2,\n",
              " ('cool', 1.0): 29,\n",
              " ('phela', 1.0): 1,\n",
              " ('obvious', 1.0): 1,\n",
              " ('princ', 1.0): 1,\n",
              " ('charm', 1.0): 1,\n",
              " ('stage', 1.0): 2,\n",
              " ('luck', 1.0): 26,\n",
              " ('tyler', 1.0): 1,\n",
              " ('hipster', 1.0): 1,\n",
              " ('glass', 1.0): 3,\n",
              " ('marti', 1.0): 2,\n",
              " ('glad', 1.0): 41,\n",
              " ('done', 1.0): 40,\n",
              " ('afternoon', 1.0): 7,\n",
              " ('let', 1.0): 12,\n",
              " ('read', 1.0): 27,\n",
              " ('kahfi', 1.0): 1,\n",
              " ('finish', 1.0): 15,\n",
              " ('ohmyg', 1.0): 1,\n",
              " ('yaya', 1.0): 3,\n",
              " ('stalk', 1.0): 2,\n",
              " ('gondooo', 1.0): 1,\n",
              " ('tologooo', 1.0): 1,\n",
              " ('becom', 1.0): 8,\n",
              " ('detail', 1.0): 8,\n",
              " ('physiotherapi', 1.0): 1,\n",
              " ('hashtag', 1.0): 3,\n",
              " ('monica', 1.0): 1,\n",
              " ('miss', 1.0): 17,\n",
              " ('sound', 1.0): 20,\n",
              " ('morn', 1.0): 65,\n",
              " (\"that'\", 1.0): 49,\n",
              " ('definit', 1.0): 20,\n",
              " ('tonight', 1.0): 15,\n",
              " ('took', 1.0): 7,\n",
              " ('advic', 1.0): 6,\n",
              " ('treviso', 1.0): 1,\n",
              " ('concert', 1.0): 22,\n",
              " ('citi', 1.0): 26,\n",
              " ('countri', 1.0): 22,\n",
              " (\"i'll\", 1.0): 74,\n",
              " ('start', 1.0): 56,\n",
              " ('fine', 1.0): 7,\n",
              " ('gorgeou', 1.0): 8,\n",
              " ('oven', 1.0): 2,\n",
              " ('roast', 1.0): 1,\n",
              " ('garlic', 1.0): 1,\n",
              " ('oliv', 1.0): 1,\n",
              " ('dri', 1.0): 2,\n",
              " ('tomato', 1.0): 1,\n",
              " ('basil', 1.0): 1,\n",
              " ('centuri', 1.0): 1,\n",
              " ('tuna', 1.0): 1,\n",
              " ('right', 1.0): 38,\n",
              " ('atchya', 1.0): 1,\n",
              " ('even', 1.0): 26,\n",
              " ('almost', 1.0): 8,\n",
              " ('chanc', 1.0): 3,\n",
              " ('cheer', 1.0): 18,\n",
              " ('cream', 1.0): 6,\n",
              " ('agre', 1.0): 13,\n",
              " ('heheheh', 1.0): 2,\n",
              " ('that', 1.0): 11,\n",
              " ('point', 1.0): 11,\n",
              " ('stay', 1.0): 21,\n",
              " ('home', 1.0): 20,\n",
              " ('soon', 1.0): 38,\n",
              " ('promis', 1.0): 4,\n",
              " ('whatsapp', 1.0): 3,\n",
              " ('volta', 1.0): 1,\n",
              " ('funcionar', 1.0): 1,\n",
              " ('iphon', 1.0): 7,\n",
              " ('jailbroken', 1.0): 1,\n",
              " ('later', 1.0): 12,\n",
              " ('min', 1.0): 6,\n",
              " ('leia', 1.0): 1,\n",
              " ('appear', 1.0): 3,\n",
              " ('hologram', 1.0): 1,\n",
              " ('r2d2', 1.0): 1,\n",
              " ('messag', 1.0): 9,\n",
              " ('sit', 1.0): 5,\n",
              " ('luke', 1.0): 4,\n",
              " ('inter', 1.0): 1,\n",
              " ('arsen', 1.0): 2,\n",
              " ('small', 1.0): 2,\n",
              " ('team', 1.0): 24,\n",
              " ('pass', 1.0): 10,\n",
              " ('dewsburi', 1.0): 2,\n",
              " ('railway', 1.0): 1,\n",
              " ('station', 1.0): 4,\n",
              " ('west', 1.0): 1,\n",
              " ('yorkshir', 1.0): 2,\n",
              " ('9:25', 1.0): 1,\n",
              " ('live', 1.0): 23,\n",
              " ('strang', 1.0): 4,\n",
              " ('imagin', 1.0): 5,\n",
              " ('megan', 1.0): 1,\n",
              " ('#masaantoday', 1.0): 4,\n",
              " ('shweta', 1.0): 1,\n",
              " ('tripathi', 1.0): 1,\n",
              " ('kurta', 1.0): 3,\n",
              " ('half', 1.0): 6,\n",
              " ('number', 1.0): 11,\n",
              " ('#wsalelov', 1.0): 14,\n",
              " ('larri', 1.0): 3,\n",
              " ('anyway', 1.0): 14,\n",
              " ('kinda', 1.0): 12,\n",
              " ('goood', 1.0): 1,\n",
              " ('life', 1.0): 36,\n",
              " ('could', 1.0): 25,\n",
              " ('warmup', 1.0): 1,\n",
              " ('15th', 1.0): 2,\n",
              " ('bath', 1.0): 6,\n",
              " ('andar', 1.0): 1,\n",
              " ('sampath', 1.0): 1,\n",
              " ('sona', 1.0): 1,\n",
              " ('mohapatra', 1.0): 1,\n",
              " ('samantha', 1.0): 1,\n",
              " ('edward', 1.0): 1,\n",
              " ('mein', 1.0): 1,\n",
              " ('tulan', 1.0): 1,\n",
              " ('razi', 1.0): 2,\n",
              " ('josh', 1.0): 1,\n",
              " ('alway', 1.0): 48,\n",
              " ('smile', 1.0): 44,\n",
              " ('pictur', 1.0): 7,\n",
              " ('16.20', 1.0): 1,\n",
              " ('#giveitup', 1.0): 1,\n",
              " ('given', 1.0): 3,\n",
              " ('subsidi', 1.0): 1,\n",
              " ('initi', 1.0): 2,\n",
              " ('propos', 1.0): 3,\n",
              " ('delight', 1.0): 4,\n",
              " ('yesterday', 1.0): 4,\n",
              " ('lmaoo', 1.0): 2,\n",
              " ('song', 1.0): 15,\n",
              " ('ever', 1.0): 19,\n",
              " ('shall', 1.0): 5,\n",
              " ('littl', 1.0): 29,\n",
              " ('throwback', 1.0): 3,\n",
              " ('outli', 1.0): 1,\n",
              " ('island', 1.0): 2,\n",
              " ('cheung', 1.0): 1,\n",
              " ('chau', 1.0): 1,\n",
              " ('total', 1.0): 6,\n",
              " ('differ', 1.0): 10,\n",
              " ('#kfckitchentour', 1.0): 2,\n",
              " ('kitchen', 1.0): 3,\n",
              " ('clean', 1.0): 1,\n",
              " ('cusp', 1.0): 1,\n",
              " ('test', 1.0): 7,\n",
              " ('water', 1.0): 7,\n",
              " ('reward', 1.0): 1,\n",
              " ('arummzz', 1.0): 2,\n",
              " (\"let'\", 1.0): 20,\n",
              " ('drive', 1.0): 9,\n",
              " ('#travel', 1.0): 7,\n",
              " ('#yogyakarta', 1.0): 3,\n",
              " ('#jeep', 1.0): 3,\n",
              " ('#indonesia', 1.0): 3,\n",
              " ('#instamood', 1.0): 3,\n",
              " ('wanna', 1.0): 23,\n",
              " ('skype', 1.0): 3,\n",
              " ('nice', 1.0): 71,\n",
              " ('friendli', 1.0): 1,\n",
              " ('pretend', 1.0): 2,\n",
              " ('film', 1.0): 8,\n",
              " ('congratul', 1.0): 9,\n",
              " ('winner', 1.0): 3,\n",
              " ('#cheesydelight', 1.0): 1,\n",
              " ('contest', 1.0): 5,\n",
              " ('address', 1.0): 8,\n",
              " ('guy', 1.0): 40,\n",
              " ('see', 1.0): 14,\n",
              " ('market', 1.0): 4,\n",
              " ('24/7', 1.0): 1,\n",
              " ('regret', 1.0): 4,\n",
              " ('hour', 1.0): 24,\n",
              " ('leav', 1.0): 12,\n",
              " ('without', 1.0): 9,\n",
              " ('delay', 1.0): 1,\n",
              " ('actual', 1.0): 12,\n",
              " ('easi', 1.0): 7,\n",
              " ('guess', 1.0): 8,\n",
              " ('train', 1.0): 7,\n",
              " ('shift', 1.0): 4,\n",
              " ('engin', 1.0): 1,\n",
              " ('sunburn', 1.0): 1,\n",
              " ('peel', 1.0): 2,\n",
              " ('blog', 1.0): 26,\n",
              " ('huge', 1.0): 9,\n",
              " ('warm', 1.0): 4,\n",
              " ('complet', 1.0): 10,\n",
              " ('triangl', 1.0): 2,\n",
              " ('northern', 1.0): 1,\n",
              " ('ireland', 1.0): 2,\n",
              " ('sight', 1.0): 1,\n",
              " ('smthng', 1.0): 2,\n",
              " ('xoxo', 1.0): 3,\n",
              " ('jaann', 1.0): 1,\n",
              " ('#topnewfollow', 1.0): 2,\n",
              " ('connect', 1.0): 13,\n",
              " ('wonder', 1.0): 26,\n",
              " ('made', 1.0): 38,\n",
              " ('fluffi', 1.0): 1,\n",
              " ('insid', 1.0): 7,\n",
              " ('pirouett', 1.0): 1,\n",
              " ('moos', 1.0): 1,\n",
              " ('trip', 1.0): 11,\n",
              " ('philli', 1.0): 1,\n",
              " ('decemb', 1.0): 2,\n",
              " ('dude', 1.0): 6,\n",
              " ('question', 1.0): 15,\n",
              " ('flaw', 1.0): 1,\n",
              " ('pain', 1.0): 8,\n",
              " ('negat', 1.0): 1,\n",
              " ('strength', 1.0): 2,\n",
              " ('went', 1.0): 10,\n",
              " ('solo', 1.0): 4,\n",
              " ('move', 1.0): 9,\n",
              " ('nirvana', 1.0): 1,\n",
              " ('smell', 1.0): 2,\n",
              " ('teen', 1.0): 1,\n",
              " ('spirit', 1.0): 1,\n",
              " ('winehous', 1.0): 1,\n",
              " ('coupl', 1.0): 5,\n",
              " ('#tomhiddleston', 1.0): 1,\n",
              " ('#elizabetholsen', 1.0): 1,\n",
              " ('#yaytheylookgreat', 1.0): 1,\n",
              " ('goodnight', 1.0): 17,\n",
              " ('wake', 1.0): 10,\n",
              " ('gonna', 1.0): 16,\n",
              " ('shoot', 1.0): 5,\n",
              " ('itti', 1.0): 2,\n",
              " ('bitti', 1.0): 2,\n",
              " ('teeni', 1.0): 2,\n",
              " ('bikini', 1.0): 3,\n",
              " ('much', 1.0): 73,\n",
              " ('get', 1.0): 29,\n",
              " ('togeth', 1.0): 6,\n",
              " ('end', 1.0): 3,\n",
              " ('xfile', 1.0): 1,\n",
              " ('content', 1.0): 3,\n",
              " ('rain', 1.0): 17,\n",
              " ('fabul', 1.0): 4,\n",
              " ('fantast', 1.0): 8,\n",
              " ('forev', 1.0): 5,\n",
              " ('belieb', 1.0): 2,\n",
              " ('nighti', 1.0): 1,\n",
              " ('bug', 1.0): 1,\n",
              " ('bite', 1.0): 1,\n",
              " ('bracelet', 1.0): 2,\n",
              " ('idea', 1.0): 24,\n",
              " ('foundri', 1.0): 1,\n",
              " ('game', 1.0): 23,\n",
              " ('sens', 1.0): 6,\n",
              " ('ef', 1.0): 1,\n",
              " ('phone', 1.0): 15,\n",
              " ('woot', 1.0): 2,\n",
              " ('derek', 1.0): 1,\n",
              " ('use', 1.0): 18,\n",
              " ('parkshar', 1.0): 1,\n",
              " ('gloucestershir', 1.0): 1,\n",
              " ('aaaahhh', 1.0): 1,\n",
              " ('traffic', 1.0): 2,\n",
              " ('stress', 1.0): 4,\n",
              " ('reliev', 1.0): 1,\n",
              " (\"how'r\", 1.0): 1,\n",
              " ('arbeloa', 1.0): 1,\n",
              " ('turn', 1.0): 14,\n",
              " ('europ', 1.0): 1,\n",
              " ('rise', 1.0): 2,\n",
              " ('find', 1.0): 22,\n",
              " ('hard', 1.0): 9,\n",
              " ('believ', 1.0): 7,\n",
              " ('uncount', 1.0): 1,\n",
              " ('unlimit', 1.0): 1,\n",
              " ('cours', 1.0): 11,\n",
              " ('#teamposit', 1.0): 1,\n",
              " ('#aldub', 1.0): 2,\n",
              " ('rita', 1.0): 2,\n",
              " ('info', 1.0): 11,\n",
              " (\"we'd\", 1.0): 4,\n",
              " ('true', 1.0): 19,\n",
              " ('sethi', 1.0): 2,\n",
              " ('high', 1.0): 6,\n",
              " ('skeem', 1.0): 1,\n",
              " ('saam', 1.0): 1,\n",
              " ('peopl', 1.0): 43,\n",
              " ('polit', 1.0): 2,\n",
              " ('izzat', 1.0): 1,\n",
              " ('wese', 1.0): 1,\n",
              " ('trust', 1.0): 7,\n",
              " ('khawateen', 1.0): 1,\n",
              " ('sath', 1.0): 2,\n",
              " ('mana', 1.0): 1,\n",
              " ('deya', 1.0): 1,\n",
              " ('sort', 1.0): 7,\n",
              " ('smart', 1.0): 5,\n",
              " ('hair', 1.0): 7,\n",
              " ('jacob', 1.0): 2,\n",
              " ('upgrad', 1.0): 2,\n",
              " ('famili', 1.0): 13,\n",
              " ('person', 1.0): 14,\n",
              " ('convers', 1.0): 6,\n",
              " ('onlin', 1.0): 4,\n",
              " ('#friday', 1.0): 10,\n",
              " ('#mclaren', 1.0): 1,\n",
              " ('#fridayfeel', 1.0): 5,\n",
              " ('#tgif', 1.0): 5,\n",
              " ('squar', 1.0): 1,\n",
              " ('enix', 1.0): 1,\n",
              " ('bissmillah', 1.0): 1,\n",
              " ('allah', 1.0): 3,\n",
              " (\"we'r\", 1.0): 26,\n",
              " ('#socent', 1.0): 1,\n",
              " ('startup', 1.0): 2,\n",
              " ('drop', 1.0): 9,\n",
              " ('your', 1.0): 4,\n",
              " ('arnd', 1.0): 1,\n",
              " ('town', 1.0): 3,\n",
              " ('basic', 1.0): 4,\n",
              " ('piss', 1.0): 2,\n",
              " ('also', 1.0): 29,\n",
              " ('terribl', 1.0): 2,\n",
              " ('complic', 1.0): 1,\n",
              " ('discuss', 1.0): 2,\n",
              " ('snapchat', 1.0): 16,\n",
              " ('lynettelow', 1.0): 1,\n",
              " ('#snapchat', 1.0): 15,\n",
              " ('#kikmenow', 1.0): 2,\n",
              " ('#snapm', 1.0): 1,\n",
              " ('#hot', 1.0): 3,\n",
              " ('#amazon', 1.0): 1,\n",
              " ('#kikmeguy', 1.0): 2,\n",
              " ('defin', 1.0): 2,\n",
              " ('grow', 1.0): 6,\n",
              " ('sport', 1.0): 4,\n",
              " ('rakyat', 1.0): 1,\n",
              " ('write', 1.0): 11,\n",
              " ('sinc', 1.0): 11,\n",
              " ('mention', 1.0): 18,\n",
              " ('fish', 1.0): 3,\n",
              " ('promot', 1.0): 3,\n",
              " ('post', 1.0): 16,\n",
              " ('cyber', 1.0): 1,\n",
              " ('#ourdaughtersourprid', 1.0): 3,\n",
              " ('#mypapamyprid', 1.0): 2,\n",
              " ('papa', 1.0): 1,\n",
              " ('coach', 1.0): 2,\n",
              " ('posit', 1.0): 3,\n",
              " ('atleast', 1.0): 2,\n",
              " ('mango', 1.0): 1,\n",
              " (\"lassi'\", 1.0): 1,\n",
              " (\"monty'\", 1.0): 1,\n",
              " ('marvel', 1.0): 1,\n",
              " ('though', 1.0): 16,\n",
              " ('suspect', 1.0): 3,\n",
              " ('meant', 1.0): 2,\n",
              " ('touch', 1.0): 7,\n",
              " ('kepler', 1.0): 3,\n",
              " ('452b', 1.0): 4,\n",
              " ('chalna', 1.0): 1,\n",
              " ('thankyou', 1.0): 10,\n",
              " ('hazel', 1.0): 1,\n",
              " ('food', 1.0): 5,\n",
              " ('brooklyn', 1.0): 1,\n",
              " ('awak', 1.0): 8,\n",
              " ('okayi', 1.0): 2,\n",
              " ('awww', 1.0): 12,\n",
              " ('splendid', 1.0): 1,\n",
              " ('spam', 1.0): 1,\n",
              " ('folder', 1.0): 1,\n",
              " ('amount', 1.0): 1,\n",
              " ('travel', 1.0): 12,\n",
              " ('nigeria', 1.0): 1,\n",
              " ('claim', 1.0): 1,\n",
              " ('rted', 1.0): 1,\n",
              " ('leg', 1.0): 2,\n",
              " ('hurt', 1.0): 4,\n",
              " ('mine', 1.0): 11,\n",
              " ('saturday', 1.0): 5,\n",
              " ('thaaank', 1.0): 1,\n",
              " ('puhon', 1.0): 1,\n",
              " ('happinesss', 1.0): 1,\n",
              " ('prior', 1.0): 1,\n",
              " ('notif', 1.0): 2,\n",
              " ('probabl', 1.0): 8,\n",
              " ('funni', 1.0): 16,\n",
              " ('2:22', 1.0): 1,\n",
              " ('yuna', 1.0): 2,\n",
              " ('tamesid', 1.0): 1,\n",
              " ('googl', 1.0): 5,\n",
              " ('account', 1.0): 17,\n",
              " ('scouser', 1.0): 1,\n",
              " ('everyth', 1.0): 10,\n",
              " ('mate', 1.0): 5,\n",
              " ('liter', 1.0): 6,\n",
              " (\"they'r\", 1.0): 10,\n",
              " ('samee', 1.0): 1,\n",
              " ('edgar', 1.0): 1,\n",
              " ('updat', 1.0): 12,\n",
              " ('bring', 1.0): 14,\n",
              " ('abe', 1.0): 1,\n",
              " ('meet', 1.0): 26,\n",
              " ('sigh', 1.0): 3,\n",
              " ('dreamili', 1.0): 1,\n",
              " ('pout', 1.0): 1,\n",
              " ('eye', 1.0): 7,\n",
              " ('#quacketyquack', 1.0): 6,\n",
              " ('happen', 1.0): 13,\n",
              " ('phil', 1.0): 1,\n",
              " ('rodder', 1.0): 1,\n",
              " ('els', 1.0): 8,\n",
              " ('play', 1.0): 37,\n",
              " ('newest', 1.0): 1,\n",
              " ('gamejam', 1.0): 1,\n",
              " ('irish', 1.0): 2,\n",
              " ('literatur', 1.0): 2,\n",
              " ('inaccess', 1.0): 2,\n",
              " (\"kareena'\", 1.0): 2,\n",
              " ('fan', 1.0): 9,\n",
              " ('brain', 1.0): 10,\n",
              " ('dot', 1.0): 8,\n",
              " ('#braindot', 1.0): 8,\n",
              " ('fair', 1.0): 4,\n",
              " ('rush', 1.0): 1,\n",
              " ('either', 1.0): 10,\n",
              " ('brandi', 1.0): 1,\n",
              " ('carniv', 1.0): 1,\n",
              " ('mask', 1.0): 2,\n",
              " ('xavier', 1.0): 1,\n",
              " ('forneret', 1.0): 1,\n",
              " ('jennif', 1.0): 1,\n",
              " ('site', 1.0): 7,\n",
              " ('free', 1.0): 31,\n",
              " ('50.000', 1.0): 3,\n",
              " ('ball', 1.0): 7,\n",
              " ('pool', 1.0): 5,\n",
              " ('coin', 1.0): 5,\n",
              " ('edit', 1.0): 6,\n",
              " ('trish', 1.0): 1,\n",
              " ('grate', 1.0): 5,\n",
              " ('three', 1.0): 8,\n",
              " ('comment', 1.0): 8,\n",
              " ('wakeup', 1.0): 1,\n",
              " ('besid', 1.0): 2,\n",
              " ('dirti', 1.0): 2,\n",
              " ('lmaooo', 1.0): 1,\n",
              " ('loui', 1.0): 4,\n",
              " (\"he'\", 1.0): 11,\n",
              " ('throw', 1.0): 3,\n",
              " ('caus', 1.0): 11,\n",
              " ('inspir', 1.0): 5,\n",
              " ('twoof', 1.0): 3,\n",
              " ('wkend', 1.0): 3,\n",
              " ('kind', 1.0): 22,\n",
              " ('exhaust', 1.0): 2,\n",
              " ('word', 1.0): 17,\n",
              " ('#cheltenham', 1.0): 1,\n",
              " ('area', 1.0): 4,\n",
              " ('werent', 1.0): 1,\n",
              " ('kale', 1.0): 1,\n",
              " ('crisp', 1.0): 1,\n",
              " ('ruin', 1.0): 5,\n",
              " ('open', 1.0): 11,\n",
              " ('worldwid', 1.0): 2,\n",
              " ('outta', 1.0): 1,\n",
              " ('#sfvbeta', 1.0): 1,\n",
              " ('vantast', 1.0): 1,\n",
              " ('xcylin', 1.0): 1,\n",
              " ('bundl', 1.0): 1,\n",
              " ('show', 1.0): 20,\n",
              " ('internet', 1.0): 2,\n",
              " ('price', 1.0): 3,\n",
              " ('realisticli', 1.0): 1,\n",
              " ('pay', 1.0): 1,\n",
              " ('educ', 1.0): 1,\n",
              " ('power', 1.0): 6,\n",
              " ('weapon', 1.0): 1,\n",
              " ('nelson', 1.0): 1,\n",
              " ('mandela', 1.0): 1,\n",
              " ('recent', 1.0): 8,\n",
              " ('chenab', 1.0): 1,\n",
              " ('flow', 1.0): 5,\n",
              " ('pakistan', 1.0): 1,\n",
              " ('#incredibleindia', 1.0): 1,\n",
              " ('#teenchoic', 1.0): 7,\n",
              " ('#choiceinternationalartist', 1.0): 7,\n",
              " ('#superjunior', 1.0): 7,\n",
              " ('caught', 1.0): 4,\n",
              " ('first', 1.0): 40,\n",
              " ('salmon', 1.0): 1,\n",
              " ('super-blend', 1.0): 1,\n",
              " ('project', 1.0): 6,\n",
              " ('youth@bipolaruk.org.uk', 1.0): 1,\n",
              " ('awesom', 1.0): 35,\n",
              " ('stream', 1.0): 12,\n",
              " ('artist', 1.0): 2,\n",
              " ('alma', 1.0): 1,\n",
              " ('mater', 1.0): 1,\n",
              " ('#highschoolday', 1.0): 1,\n",
              " ('#clientvisit', 1.0): 1,\n",
              " ('faith', 1.0): 3,\n",
              " ('christian', 1.0): 1,\n",
              " ('school', 1.0): 9,\n",
              " ('#lizaminnelli', 1.0): 1,\n",
              " ('upcom', 1.0): 2,\n",
              " ('singl', 1.0): 4,\n",
              " ('hill', 1.0): 4,\n",
              " ('everi', 1.0): 23,\n",
              " ('beat', 1.0): 7,\n",
              " ('wrong', 1.0): 9,\n",
              " ('readi', 1.0): 22,\n",
              " ('natur', 1.0): 1,\n",
              " ('pefumeri', 1.0): 1,\n",
              " ('workshop', 1.0): 2,\n",
              " ('neal', 1.0): 1,\n",
              " ('yard', 1.0): 1,\n",
              " ('covent', 1.0): 1,\n",
              " ('tomorrow', 1.0): 31,\n",
              " ('fback', 1.0): 26,\n",
              " ('indo', 1.0): 1,\n",
              " ('harmo', 1.0): 1,\n",
              " ('americano', 1.0): 1,\n",
              " ('rememb', 1.0): 9,\n",
              " ('head', 1.0): 12,\n",
              " ('dark', 1.0): 5,\n",
              " ('handshom', 1.0): 1,\n",
              " ('juga', 1.0): 1,\n",
              " ('hurray', 1.0): 1,\n",
              " ('hate', 1.0): 9,\n",
              " ('cant', 1.0): 13,\n",
              " ('decid', 1.0): 2,\n",
              " ('save', 1.0): 7,\n",
              " ('list', 1.0): 10,\n",
              " ('hiya', 1.0): 3,\n",
              " ('exec', 1.0): 1,\n",
              " ('loryn.good@lincs-chamber.co.uk', 1.0): 1,\n",
              " ('photo', 1.0): 16,\n",
              " ('china', 1.0): 2,\n",
              " ('homosexu', 1.0): 1,\n",
              " ('#hyungbot', 1.0): 1,\n",
              " ('give', 1.0): 42,\n",
              " ('mind', 1.0): 13,\n",
              " ('#jgh', 1.0): 1,\n",
              " ('#timetunnel', 1.0): 1,\n",
              " ('1982', 1.0): 1,\n",
              " ('quit', 1.0): 11,\n",
              " ('radio', 1.0): 5,\n",
              " ('heart', 1.0): 6,\n",
              " ('hiii', 1.0): 2,\n",
              " ('jack', 1.0): 3,\n",
              " ('domino', 1.0): 1,\n",
              " ('heat', 1.0): 2,\n",
              " ('prob', 1.0): 5,\n",
              " ('sorri', 1.0): 12,\n",
              " ('hastili', 1.0): 1,\n",
              " ('type', 1.0): 3,\n",
              " ('came', 1.0): 7,\n",
              " ('pakistani', 1.0): 1,\n",
              " ('3point', 1.0): 1,\n",
              " ('#dreamteam', 1.0): 1,\n",
              " ('gooo', 1.0): 1,\n",
              " ('bailey', 1.0): 2,\n",
              " ('#pbb737gold', 1.0): 3,\n",
              " ('drank', 1.0): 1,\n",
              " ('gotten', 1.0): 2,\n",
              " ('#welsh', 1.0): 1,\n",
              " ('#wale', 1.0): 1,\n",
              " ('yippe', 1.0): 1,\n",
              " ('lord', 1.0): 3,\n",
              " ('michael', 1.0): 3,\n",
              " (\"u'r\", 1.0): 1,\n",
              " ('bigot', 1.0): 1,\n",
              " ('usual', 1.0): 6,\n",
              " ('front', 1.0): 3,\n",
              " ('squat', 1.0): 1,\n",
              " ('dobar', 1.0): 1,\n",
              " ('brand', 1.0): 5,\n",
              " ('heavi', 1.0): 2,\n",
              " ('#musicology2015', 1.0): 1,\n",
              " ('#day2', 1.0): 1,\n",
              " ('spend', 1.0): 1,\n",
              " ('marathon', 1.0): 1,\n",
              " ('iflix', 1.0): 1,\n",
              " ('offici', 1.0): 8,\n",
              " ('graduat', 1.0): 2,\n",
              " ('expert', 1.0): 1,\n",
              " ('bisexu', 1.0): 1,\n",
              " ('minal', 1.0): 1,\n",
              " ('aidzin', 1.0): 1,\n",
              " ('cook', 1.0): 1,\n",
              " ('book', 1.0): 15,\n",
              " ('dinner', 1.0): 5,\n",
              " ('tough', 1.0): 2,\n",
              " ('choic', 1.0): 6,\n",
              " ('other', 1.0): 8,\n",
              " ('chill', 1.0): 5,\n",
              " ('oval', 1.0): 1,\n",
              " ('basketbal', 1.0): 1,\n",
              " ('player', 1.0): 4,\n",
              " ('whahahaha', 1.0): 1,\n",
              " ('#soamaz', 1.0): 1,\n",
              " ('moment', 1.0): 10,\n",
              " ('onto', 1.0): 3,\n",
              " ('wardrob', 1.0): 2,\n",
              " ('user', 1.0): 3,\n",
              " ('#teamr', 1.0): 1,\n",
              " ('appar', 1.0): 5,\n",
              " ('depend', 1.0): 2,\n",
              " ('greatli', 1.0): 1,\n",
              " ('design', 1.0): 18,\n",
              " ('ahhh', 1.0): 1,\n",
              " ('cinepambata', 1.0): 1,\n",
              " ('mechan', 1.0): 1,\n",
              " ('form', 1.0): 3,\n",
              " ('download', 1.0): 10,\n",
              " ('sali', 1.0): 1,\n",
              " ('mom', 1.0): 1,\n",
              " ('swisher', 1.0): 1,\n",
              " ('cop', 1.0): 1,\n",
              " ('ducktail', 1.0): 1,\n",
              " ('surreal', 1.0): 3,\n",
              " ('exposur', 1.0): 1,\n",
              " ('#sotw', 1.0): 1,\n",
              " ('jingli', 1.0): 1,\n",
              " ('jangli', 1.0): 1,\n",
              " ('loveli', 1.0): 1,\n",
              " ('#halesowen', 1.0): 1,\n",
              " ('#blackcountryfair', 1.0): 1,\n",
              " ('street', 1.0): 1,\n",
              " ('assess', 1.0): 1,\n",
              " ('mental', 1.0): 1,\n",
              " ('bodi', 1.0): 12,\n",
              " ('ooz', 1.0): 1,\n",
              " ('appeal', 1.0): 1,\n",
              " ('amassiveoverdoseofship', 1.0): 1,\n",
              " ('latest', 1.0): 4,\n",
              " ('isi', 1.0): 1,\n",
              " ('chan', 1.0): 1,\n",
              " ('note', 1.0): 4,\n",
              " ('#pkwalasawa', 1.0): 1,\n",
              " ('gemma', 1.0): 1,\n",
              " ('orlean', 1.0): 1,\n",
              " ('#fever', 1.0): 1,\n",
              " ('#geskenya', 1.0): 1,\n",
              " ('#obamainkenya', 1.0): 1,\n",
              " ('#magicalkenya', 1.0): 1,\n",
              " ('#greatkenya', 1.0): 1,\n",
              " ('#allgoodthingsk', 1.0): 1,\n",
              " ('anim', 1.0): 6,\n",
              " ('umaru', 1.0): 1,\n",
              " ('singer', 1.0): 1,\n",
              " ('ship', 1.0): 6,\n",
              " ('order', 1.0): 12,\n",
              " ('room', 1.0): 4,\n",
              " ('gone', 1.0): 4,\n",
              " ('hahaha', 1.0): 11,\n",
              " ('stori', 1.0): 10,\n",
              " ('relat', 1.0): 2,\n",
              " ('label', 1.0): 1,\n",
              " ('worst', 1.0): 3,\n",
              " ('batch', 1.0): 1,\n",
              " ('princip', 1.0): 1,\n",
              " ('march', 1.0): 1,\n",
              " ('wooftast', 1.0): 2,\n",
              " ('receiv', 1.0): 7,\n",
              " ('necessari', 1.0): 1,\n",
              " ('whatev', 1.0): 4,\n",
              " ('success', 1.0): 5,\n",
              " ('abstin', 1.0): 1,\n",
              " (\"there'\", 1.0): 6,\n",
              " ('thrown', 1.0): 1,\n",
              " ('middl', 1.0): 2,\n",
              " ('repeat', 1.0): 3,\n",
              " ('relentlessli', 1.0): 1,\n",
              " ('approxim', 1.0): 1,\n",
              " ('oldschool', 1.0): 1,\n",
              " ('runescap', 1.0): 1,\n",
              " ('daaay', 1.0): 1,\n",
              " ('#jumma_mubarik', 1.0): 1,\n",
              " ('#frnd', 1.0): 1,\n",
              " ('#stay_bless', 1.0): 1,\n",
              " ('bless', 1.0): 8,\n",
              " ('pussycat', 1.0): 1,\n",
              " ('main', 1.0): 7,\n",
              " ('launch', 1.0): 4,\n",
              " ('pretoria', 1.0): 1,\n",
              " ('#fahrinahmad', 1.0): 1,\n",
              " ('#tengkuaaronshah', 1.0): 1,\n",
              " ('#eksperimencinta', 1.0): 1,\n",
              " ('tykksin', 1.0): 1,\n",
              " ('videosta', 1.0): 1,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV5_4qVUKewa",
        "outputId": "c9d3e69b-a1b5-4885-83eb-aac554d0fbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 9828\n"
          ]
        }
      ],
      "source": [
        "# check out the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVQ7YMoKp2T",
        "outputId": "06ce2357-c7aa-4d37-c348-8173870ac582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of a positive tweet: \n",
            " @gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['yeah', 'suppos', 'chat']\n"
          ]
        }
      ],
      "source": [
        "# test the function below\n",
        "\n",
        "print('This is an example of a positive tweet: \\n', train_x[22])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[22]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCaQzncSKwL_"
      },
      "outputs": [],
      "source": [
        "# Unlike most, we will actully build a Logistic Regression Model from scratch\n",
        "#  Logistic regression\n",
        "\n",
        "# Sigmoid Function\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    \"\"\"\n",
        "    zz = np.negative(z)\n",
        "    h = 1 / (1 + np.exp(zz))\n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARg-_KLtK-ZX"
      },
      "outputs": [],
      "source": [
        "# Cost function and Gradient\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    \"\"\"\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "    for i in range(0, num_iters):\n",
        "        z = np.dot(x, theta)\n",
        "        h = sigmoid(z)\n",
        "        # calculate the cost function\n",
        "        cost = -1. / m * (np.dot(y.transpose(), np.log(h)) + np.dot((1 - y).transpose(), np.log(1 - h)))\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha / m) * np.dot(x.transpose(), (h - y))\n",
        "\n",
        "    cost = float(cost)\n",
        "    return cost, theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3RneTodLBcC"
      },
      "outputs": [],
      "source": [
        "#  Extracting the features\n",
        "\n",
        "def extract_features(tweet, freqs):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    \"\"\"\n",
        "\n",
        "    word_l = process_tweet(tweet)\n",
        "    x = np.zeros((1, 3))\n",
        "\n",
        "    # bias term is set to 1\n",
        "    x[0, 0] = 1\n",
        "\n",
        "    for word in word_l:\n",
        "        # increment the word count for the positive label 1\n",
        "        x[0, 1] += freqs.get((word, 1.0), 0)\n",
        "        # increment the word count for the negative label 0\n",
        "        x[0, 2] += freqs.get((word, 0.0), 0)\n",
        "\n",
        "    assert (x.shape == (1, 3))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x[22]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_9d2J9zfC-Em",
        "outputId": "cf8b5ff0-10fc-46ca-f5a6-92cd88dbf24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSTFB_uCLEdz",
        "outputId": "47e72fa7-b2a5-487e-97c6-59ee108b000c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1. 43. 42.]]\n"
          ]
        }
      ],
      "source": [
        "# test on training data\n",
        "\n",
        "tmp1 = extract_features(train_x[22], freqs)\n",
        "print(tmp1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRFwSxLILIDI"
      },
      "outputs": [],
      "source": [
        "# Try to understand what all these three numbers mean.\n",
        "# Usually we get a dataset with a lot of features/columns, here we just have text data.\n",
        "# Those three numbers are the feature set that we have build using build_freq() and extract_features() function.\n",
        "# build_freq() builds a dictionary having words as keys and the number of times they have occurred in corpus as values.\n",
        "# Extract feature takes in sum of these values for positive and negative words, i.e. tmp1[1] and tmp[2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiHmtVaIN5KB"
      },
      "outputs": [],
      "source": [
        "# How these features will be used to predict in Logistic Regression\n",
        "\n",
        "# First a hypothesis is build which for our case will be h(x) = b1 + b2*x1 + b3*x2\n",
        "# here b1 = 1, b2 and b3 are determined by cost and gradient function, x1 and x2 are the positive and negative words feature set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "antu04K8Dzp4",
        "outputId": "c6cbcd15-5b50-4fb4-b4a6-c5b526f85987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       ...,\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lUutIScOWj7",
        "outputId": "1e3da481-1a56-4c2f-e265-cf45aa2127a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-36ceb5ce0c00>:25: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  cost = float(cost)\n"
          ]
        }
      ],
      "source": [
        "# Training Your Model\n",
        "\n",
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "# these values are predefined (Andrew NG)\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUJ-lXxfOcvJ"
      },
      "outputs": [],
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    \"\"\"\n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet, freqs)\n",
        "    y_pred = sigmoid(np.dot(x, theta))\n",
        "\n",
        "    return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDQP49HuOhjt"
      },
      "outputs": [],
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "\n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        if y_pred > 0.5:\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            y_hat.append(0)\n",
        "\n",
        "    accuracy = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-B599jISbFd",
        "outputId": "9d8bac8b-3214-4d41-c46f-52ae5f33d955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.5700\n"
          ]
        }
      ],
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1RuRdapSdYB"
      },
      "outputs": [],
      "source": [
        "# Predict with your own tweet\n",
        "\n",
        "# def pre(sentence):\n",
        "#     yhat = predict_tweet(sentence, freqs, theta)\n",
        "#     if yhat > 0.5:\n",
        "#         return 'Positive sentiment'\n",
        "#     elif yhat == 0:\n",
        "#         return 'Neutral sentiment'\n",
        "#     else:\n",
        "#         return 'Negative sentiment'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def pre(sentence):\n",
        "    yhat = predict_tweet(sentence, freqs, theta)\n",
        "    if yhat > 0.5:\n",
        "        return 'Positive sentiment'\n",
        "    elif yhat == 0:\n",
        "        return 'Neutral sentiment'\n",
        "    else:\n",
        "        return 'Negative sentiment'\n",
        "\n",
        "iface = gr.Interface(fn=pre, inputs=\"text\", outputs=\"text\")\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "_SDjSPyq3BxQ",
        "outputId": "5eadc689-6c59-4176-a0ca-4bb8b71c6178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://304253d1d8374bd955.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://304253d1d8374bd955.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfaCSbBzSoG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f9d821-8579-4568-cc22-68b1ed1ee438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative sentiment\n"
          ]
        }
      ],
      "source": [
        "my_tweet = 'criminal'\n",
        "\n",
        "res = pre(my_tweet)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LdyAf8mSs0P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}